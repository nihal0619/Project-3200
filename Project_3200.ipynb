{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/JZn6nL6Ls5NXu0KSIYcZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nihal0619/Project-3200/blob/main/Project_3200.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IdQG5ZNh_s5K"
      },
      "outputs": [],
      "source": [
        "!wget -q https://git.io/J0fjL -O IAM_Words.zip\n",
        "!unzip -qq IAM_Words.zip\n",
        "!mkdir data\n",
        "!mkdir data/words\n",
        "!tar -xf IAM_Words/words.tgz -C data/words\n",
        "!mv IAM_Words/words.txt data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -30 data/words.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_QEfzYZBRpK",
        "outputId": "a7466d57-cfa8-4e3e-9251-5beb2ca4af6c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#--- words.txt ---------------------------------------------------------------#\n",
            "#\n",
            "# iam database word information\n",
            "#\n",
            "# format: a01-000u-00-00 ok 154 1 408 768 27 51 AT A\n",
            "#\n",
            "#     a01-000u-00-00  -> word id for line 00 in form a01-000u\n",
            "#     ok              -> result of word segmentation\n",
            "#                            ok: word was correctly\n",
            "#                            er: segmentation of word can be bad\n",
            "#\n",
            "#     154             -> graylevel to binarize the line containing this word\n",
            "#     1               -> number of components for this word\n",
            "#     408 768 27 51   -> bounding box around this word in x,y,w,h format\n",
            "#     AT              -> the grammatical tag for this word, see the\n",
            "#                        file tagset.txt for an explanation\n",
            "#     A               -> the transcription for this word\n",
            "#\n",
            "a01-000u-00-00 ok 154 408 768 27 51 AT A\n",
            "a01-000u-00-01 ok 154 507 766 213 48 NN MOVE\n",
            "a01-000u-00-02 ok 154 796 764 70 50 TO to\n",
            "a01-000u-00-03 ok 154 919 757 166 78 VB stop\n",
            "a01-000u-00-04 ok 154 1185 754 126 61 NPT Mr.\n",
            "a01-000u-00-05 ok 154 1438 746 382 73 NP Gaitskell\n",
            "a01-000u-00-06 ok 154 1896 757 173 72 IN from\n",
            "a01-000u-01-00 ok 156 395 932 441 100 VBG nominating\n",
            "a01-000u-01-01 ok 156 901 958 147 79 DTI any\n",
            "a01-000u-01-02 ok 156 1112 958 208 42 AP more\n",
            "a01-000u-01-03 ok 156 1400 937 294 59 NN Labour\n",
            "a01-000u-01-04 ok 156 1779 932 174 63 NN life\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "I5xbWc14B83C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path =\"data\"\n",
        "words_list = []\n",
        "\n",
        "words = open(f\"{base_path}/words.txt\", \"r\").readlines()\n",
        "for line in words:\n",
        "     if line[0] == \"#\" :\n",
        "       continue\n",
        "\n",
        "     if line.split(\" \")[1] != \"err\" :\n",
        "       words_list.append(line)\n",
        "\n",
        "len (words_list)\n",
        "np.random.shuffle(words_list)      "
      ],
      "metadata": {
        "id": "ZFWg0JGuCz4a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_idx = int(0.9 * len(words_list))\n",
        "train_samples = words_list[:split_idx]\n",
        "test_samples = words_list [split_idx:]\n",
        "\n",
        "val_split_idx = int (0.5 *len(test_samples))\n",
        "validation_samples =test_samples [:val_split_idx]\n",
        "test_samples = test_samples[val_split_idx:]\n",
        "\n",
        "assert len(words_list) == len(train_samples) + len(validation_samples) + len(test_samples)\n",
        "\n",
        "print(f\"total training samples: {len(train_samples)}\")\n",
        "print(f\"total validation samples : {len(validation_samples)}\")\n",
        "print(f\"total test samples: {len(test_samples)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7WGLvoeEQn_",
        "outputId": "daf7a4a8-d6f1-46f6-ecbb-3dc71e200ffd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training samples: 86810\n",
            "total validation samples : 4823\n",
            "total test samples: 4823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_image_path = os.path.join(base_path, \"words\")\n",
        "\n",
        "def get_image_paths_and_labels(samples):\n",
        "    paths = []\n",
        "    corrected_samples = []\n",
        "    for (i, file_line) in enumerate(samples):\n",
        "        line_split = file_line.strip()\n",
        "        line_split = line_split.split(\" \") \n",
        "        #each line split will have the following format\n",
        "        #part1/part1.part2/part1.part2.part3.png\n",
        "        image_name = line_split[0]\n",
        "        partI = image_name.split(\"-\")[0]\n",
        "        partII = image_name.split(\"-\")[1]\n",
        "        img_path = os.path.join (\n",
        "            base_image_path, partI, partI + \"-\" + partII, image_name + \".png\"\n",
        "        )\n",
        "        if os.path.getsize( img_path):\n",
        "          paths.append(img_path)\n",
        "          corrected_samples.append(file_line.split(\"\\n\")[0])\n",
        "     \n",
        "    return paths, corrected_samples    \n",
        "\n",
        "\n",
        "train_img_paths, train_labels = get_image_paths_and_labels(train_samples)\n",
        "validation_img_paths, validation_labels = get_image_paths_and_labels(validation_samples)     \n",
        "test_img_paths, test_labels = get_image_paths_and_labels(test_samples)\n",
        " \n",
        "      "
      ],
      "metadata": {
        "id": "X_vyFh354MCD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Determne the max length and the size of the vocabulary in the trainning data.\n",
        "train_labels_cleaned = []\n",
        "characters = set ()\n",
        "max_len = 0\n",
        "\n",
        "for label in train_labels:\n",
        "  label = label.split(\" \")[-1].strip()\n",
        "  for char in label:\n",
        "    characters.add(char)\n",
        "\n",
        "  max_len = max(max_len, len(label))\n",
        "  train_labels_cleaned.append(label)\n",
        "\n",
        "print(\"max length \" , max_len)\n",
        "print(\"Vocab size \" , len(characters))\n",
        "\n",
        "#check some label samples\n",
        "train_labels_cleaned[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1Vltz32tZsN",
        "outputId": "ca312c46-6e78-442a-dceb-4d8fbe180cd9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max length  21\n",
            "Vocab size  78\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sure',\n",
              " 'he',\n",
              " 'during',\n",
              " 'of',\n",
              " 'booty',\n",
              " 'gastronomy',\n",
              " 'boy',\n",
              " 'The',\n",
              " 'and',\n",
              " 'in']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_lables(labels):\n",
        "    cleaned_labels = []\n",
        "    for label in labels:\n",
        "      label = label.split(\" \")[-1].strip()\n",
        "      cleaned_labels.append(label)\n",
        "    return cleaned_labels\n",
        "\n",
        "\n",
        "validation_labels_cleaned = clean_lables(validation_labels)\n",
        "test_labels_cleaned = clean_lables(test_labels)"
      ],
      "metadata": {
        "id": "x2O3brUmtdPk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# map characters into integers.\n",
        "char_to_num = StringLookup(vocabulary=list(characters), mask_token=None)\n",
        "\n",
        "# map integers back to original characters \n",
        "num_to_char = StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
        ")"
      ],
      "metadata": {
        "id": "pjqWIuvDtf6V"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}